{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f10a5427",
   "metadata": {},
   "source": [
    "# 回归分析 Regression Analysis\n",
    "\n",
    "xyfJASON\n",
    "\n",
    "在「插值与拟合」中我们已经了解了最小二乘法作曲线拟合，但是从数理统计的观点看，数据点的观测具有误差，可以视为**随机变量**，我们有必要对结果作区间估计或假设检验，以评估结果的可信度和模型的优劣。**简单地说，回归分析就是对拟合问题作统计分析。**\n",
    "\n",
    "回归分析会研究以下几个问题：\n",
    "1. 建立因变量 $y$ 和自变量 $x_1,x_2,\\ldots,x_m$ 之间的回归模型；\n",
    "2. 检验回归模型的可信度（拟合效果）；\n",
    "3. 检验每个自变量 $x_i$ 对 $y$ 的影响是否显著；\n",
    "4. 判断回归模型是否适合样本数据；\n",
    "5. 使用回归模型"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad062018",
   "metadata": {},
   "source": [
    "## 1 一元线性回归\n",
    "\n",
    "### 1.1 模型\n",
    "\n",
    "一元线性回归的模型为：\n",
    "\n",
    "$$\n",
    "y=\\beta_0+\\beta_1x+\\epsilon\n",
    "$$\n",
    "\n",
    "其中 $\\beta_0,\\beta_1$ 是回归系数，$\\epsilon\\sim N(0, \\sigma^2)$ 是随机误差项，故随机变量 $y\\sim N(\\beta_0+\\beta_1x, \\sigma^2)$。\n",
    "\n",
    "设我们进行了 $n$ 次观测，得到样本 $\\{x_i,y_i\\},\\,i=1,2,\\ldots,n$，它们符合模型：$y_i=\\beta_0+\\beta_1x_i+\\epsilon_i$，且 $\\epsilon_i$ 之间相互独立。\n",
    "\n",
    "### 1.2 最小二乘估计\n",
    "\n",
    "#### 1.2.1 最小二乘法\n",
    "\n",
    "最小二乘估计即取 $\\beta_0,\\beta_1$ 的一组估计值 $\\hat\\beta_0,\\hat\\beta_1$，使得误差平方和最小：\n",
    "\n",
    "$$\n",
    "\\hat\\beta_0,\\hat\\beta_1=\\arg\\min_{\\beta_0,\\beta_1}\\sum_{i=1}^n\\left(y_i-\\beta_0-\\beta_1x_i\\right)^2\n",
    "$$\n",
    "\n",
    "令偏导为零，解方程可得：\n",
    "\n",
    "$$\n",
    "\\hat\\beta_1=\\frac{\\sum\\limits_{i=1}^n(x_i-\\bar x)(y_i-\\bar y)}{\\sum\\limits_{i=1}^n(x_i-\\bar x)^2},\\quad\\hat\\beta_0=\\bar y-\\hat\\beta_1\\bar x\\tag{1}\n",
    "$$\n",
    "\n",
    "其中 $\\bar x=\\frac{1}{n}\\sum\\limits_{i=1}^nx_i,\\,\\bar y=\\frac{1}{n}\\sum\\limits_{i=1}^ny_i$。\n",
    "\n",
    "也可以改写为：\n",
    "\n",
    "$$\n",
    "\\hat\\beta_1=\\frac{s_y}{s_x}r_{xy}\\tag{2}\n",
    "$$\n",
    "\n",
    "其中 $s_x^2=\\frac{1}{n-1}\\sum\\limits_{i=1}^n(x_i-\\bar x)^2,\\,s_y^2=\\frac{1}{n-1}\\sum\\limits_{i=1}^n(y_i-\\bar y)^2$ 是样本方差，$r_{xy}=\\cfrac{\\sum\\limits_{i=1}^n(x_i-\\bar x)(y_i-\\bar y)}{\\sqrt{\\sum\\limits_{i=1}^n(x_i-\\bar x)^2}\\sqrt{\\sum\\limits_{i=1}^n(y_i-\\bar y)^2}}$ 是 $x$ 与 $y$ 的样本相关系数。\n",
    "\n",
    "特别地，当 $x,y$ 均已标准化时，$\\bar x=\\bar y=0,\\,s_x=s_y=1$，于是回归方程为：\n",
    "\n",
    "$$\n",
    "\\hat y=r_{xy}x\n",
    "$$\n",
    "\n",
    "#### 1.2.2 $\\hat\\beta_1$ 的性质\n",
    "\n",
    "注意 $\\hat\\beta_1$ 是一个随机变量，它具有以下性质：\n",
    "1. $\\hat\\beta_1$ 可以写作 $y_i$ 的线性组合，即 $\\hat\\beta_1=\\sum\\limits_{i=1}^nk_iy_i$，其中 $k_i=\\cfrac{x_i-\\bar x}{\\sum\\limits_{j=1}^n(x_j-\\bar x)^2}$；\n",
    "2. 由于 $y_i$ 是相互独立的正态随机变量，所以 $\\hat\\beta_1$ 也是正态随机变量；\n",
    "3. 点估计量 $\\hat\\beta_1$ 是真值 $\\beta_1$ 的无偏估计，即 $\\mathbb E[\\hat \\beta_1]=\\beta_1$；\n",
    "4. 点估计量 $\\hat\\beta_1$ 的方差为：$\\text{var}(\\hat\\beta_1)=\\cfrac{\\sigma^2}{\\sum\\limits_{i=1}^n(x_i-\\bar x)^2}$\n",
    "\n",
    "#### 1.2.3 其他性质\n",
    "\n",
    "最小二乘还具有一些值得注意的性质：\n",
    "1. 残差和为零：$\\sum\\limits_{i=1}^ne_i=\\sum\\limits_{i=1}^n(y_i-\\hat y_i)=0$\n",
    "2. 拟合值 $\\hat y_i$ 的平均值等于观测值 $y_i$ 的平均值：$\\frac{1}{n}\\sum\\limits_{i=1}^n\\hat y_i=\\frac{1}{n}\\sum\\limits_{i=1}^n y_i=\\bar y$\n",
    "3. $\\sum\\limits_{i=1}^nx_ie_i=0$\n",
    "4. $\\sum\\limits_{i=1}^n\\hat y_ie_i=0$\n",
    "5. 回归直线总是过 $(\\bar x, \\bar y)$\n",
    "\n",
    "\n",
    "\n",
    "### 1.3 拟合效果分析\n",
    "\n",
    "#### 1.3.1 残差的样本方差\n",
    "\n",
    "残差：$e_i=y_i-\\hat y_i$，其样本均值为：$\\frac{1}{n}\\sum\\limits_{i=1}^n(y_i-\\hat y_i)=0$，其样本方差（也即均方误差）为：\n",
    "\n",
    "$$\n",
    "\\text{MSE}=\\frac{1}{n-2}\\sum_{i=1}^ne_i^2=\\frac{1}{n-2}\\sum_{i=1}^n(y_i-\\hat y_i)^2\n",
    "$$\n",
    "\n",
    "（由于有两个约束：$\\sum_{i=1}^ne_i=0,\\,\\sum_{i=1}^nx_ie_i=0$，所以自由度为 $n-2$）$\\text{MSE}$ 是总体方差 $\\sigma^2=\\text{var}(\\epsilon_i)$ 的无偏估计量。\n",
    "\n",
    "#### 1.3.2 判定系数\n",
    "\n",
    "不同的 $x_i$ 对应不同的 $y_i$，建立一元线性回归模型，就是试图用 $x$ 的线性函数解释 $y$ 的变异。因此我们需要判定回归模型 $\\hat y=\\hat\\beta_1x+\\hat \\beta_0$ 究竟能以多大精度解释 $y$ 的变异。\n",
    "\n",
    "$y$ 的变异可以由样本方差刻画：\n",
    "\n",
    "$$\n",
    "s^2=\\frac{1}{n-1}\\sum_{i=1}^n(y_i-\\bar y)^2\n",
    "$$\n",
    "\n",
    "根据前述性质，拟合值 $\\hat y_i$ 的均值也是 $\\bar y$，故其变异程度可以类似地刻画：\n",
    "\n",
    "$$\n",
    "\\hat s^2=\\frac{1}{n-1}\\sum_{i=1}^n(\\hat y_i-\\bar y)^2\n",
    "$$\n",
    "\n",
    "上述二者的关系是：\n",
    "\n",
    "$$\n",
    "\\sum_{i=1}^n(y_i-\\bar y)^2=\\sum_{i=1}^n(\\hat y_i-\\bar y)^2+\\sum_{i=1}^n(y_i-\\hat y_i)^2\n",
    "$$\n",
    "\n",
    "上式中第一项 $\\text{SST}=\\sum\\limits_{i=1}^n(y_i-\\bar y)^2$ 是原始数据的变异程度；第二项 $\\text{SSR}=\\sum\\limits_{i=1}^n(\\hat y_i-\\bar y)^2$ 是拟合数据的变异程度；第三项 $\\text{SSE}=\\sum\\limits_{i=1}^n(y_i-\\hat y_i)^2$ 是残差平方和。\n",
    "\n",
    "对于一个确定的样本，$\\text{SST}$ 是固定的，$\\text{SSR}$ 越大，说明回归方程能越好地解释原数据的变异；同时 $\\text{SSE}$ 越小，说明回归方程对原数据拟合得越好。\n",
    "\n",
    "定义**判定系数**：\n",
    "\n",
    "$$\n",
    "R^2=\\frac{\\text{SSR}}{\\text{SST}}=1-\\frac{\\text{SSE}}{\\text{SST}}\n",
    "$$\n",
    "\n",
    "可以知道 $R^2\\in[0,1]$，且其数值越大，表明拟合得越好。事实上，我们可以推导出来：\n",
    "\n",
    "$$\n",
    "R^2=r^2(y,\\hat y)\n",
    "$$\n",
    "\n",
    "即 $\\sqrt{R^2}$ 是 $y,\\,\\hat y$ 的相关系数。\n",
    "\n",
    "\n",
    "\n",
    "### 1.4 显著性检验\n",
    "\n",
    "若我们想检验 $x$ 是否对 $y$ 有显著影响，可以用单因素方差分析作 F 检验，详见 `Analysis of Variance.ipynb`。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64886698",
   "metadata": {},
   "source": [
    "## 2 多元线性回归\n",
    "\n",
    "### 2.1 模型\n",
    "\n",
    "多元回归分析的模型为：\n",
    "\n",
    "$$\n",
    "y=\\beta_0+\\beta_1x_1+\\cdots+\\beta_mx_m+\\epsilon\n",
    "$$\n",
    "\n",
    "其中，$\\beta_0,\\ldots, \\beta_m$ 是回归系数，$\\epsilon\\sim N(0, \\sigma^2$ 是随机误差项。\n",
    "\n",
    "设我们进行了 $n$ 次观测，得到数据：$(y_i, x_{i1},\\ldots,x_{im}),\\,i=1,2,\\ldots,n$，则：\n",
    "\n",
    "$$\n",
    "y_i=\\beta_0+\\beta_1x_{i1}+\\cdots+\\beta_mx_{im}+\\epsilon_i,\\,i=1,2,\\ldots,n\n",
    "$$\n",
    "\n",
    "若令：\n",
    "\n",
    "$$\n",
    "Y=\\begin{bmatrix}y_1\\\\\\vdots\\\\y_n\\end{bmatrix},\\,\n",
    "X=\\begin{bmatrix}\n",
    "1&x_{11}&\\cdots&x_{1m}\\\\\n",
    "\\vdots&\\vdots&\\ddots&\\vdots\\\\\n",
    "1&x_{n1}&\\cdots&x_{nm}\n",
    "\\end{bmatrix},\\,\n",
    "\\epsilon=\\begin{bmatrix}\\epsilon_1\\\\\\vdots\\\\\\epsilon_n\\end{bmatrix},\\,\n",
    "\\beta=\\begin{bmatrix}\\beta_0\\\\\\beta_1\\\\\\vdots\\\\\\beta_m\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "那么上式可以表示为：\n",
    "\n",
    "$$\n",
    "Y=X\\beta+\\epsilon\n",
    "$$\n",
    "\n",
    "### 2.2 最小二乘估计\n",
    "\n",
    "仍然使用最小二乘法，使得误差平方和最小：\n",
    "\n",
    "$$\n",
    "\\hat\\beta=\\arg\\min_\\beta\\sum_{i=1}^n(y_i-\\beta\\cdot x_i)^2\n",
    "$$\n",
    "\n",
    "可以解得（若 $X$ 满秩）：\n",
    "\n",
    "$$\n",
    "\\hat\\beta=(X^TX)^{-1}X^TY\n",
    "$$\n",
    "\n",
    "于是数据的拟合值为 $\\hat Y=X\\hat\\beta$，残差为 $e=Y-\\hat Y$，残差平方和为 $Q=\\sum\\limits_{i=1}^ne_i^2=\\sum\\limits_{i=1}^n(y_i-\\hat y_i)^2$\n",
    "\n",
    "### 2.3 统计分析\n",
    "\n",
    "1. $\\hat\\beta$ 是 $\\beta$ 的线性无偏最小方差估计；\n",
    "2. $\\beta\\sim N(\\beta, \\sigma^2(X^TX)^{-1})$\n",
    "3. $\\mathbb EQ=(n-m-1)\\sigma^2$，$\\frac{Q}{\\sigma^2}\\sim \\chi^2(n-m-1)$，由此得到 $\\sigma^2$ 的无偏估计：$s^2=\\frac{Q}{n-m-1}=\\hat\\sigma^2$，称 $s^2$ 为剩余方差，$s$ 为剩余标准差；\n",
    "\n",
    "### 2.4 假设检验\n",
    "\n",
    "对样本点 $y_i$ 作单因素方差分析，可以检验假设 $H_0:\\beta_1=\\cdots=\\beta_m=0$。\n",
    "\n",
    "当上述 $H_0$ 被拒绝时，只能说明 $\\beta_j$ 不全为零，不能排除其中若干个为零。如果要对每一个 $\\beta_j$ 进行判断，应该作下述 $m+1$ 个 $t$ 检验：\n",
    "\n",
    "设原假设 $H_0^{(j)}:\\beta_j=0$，则当 $H_0^{(j)}$ 成立时，有：\n",
    "\n",
    "$$\n",
    "t_j=\\frac{\\hat\\beta_j/\\sqrt{c_{jj}}}{\\sqrt{Q/(n-m-1)}}\\sim t(n-m-1)\n",
    "$$\n",
    "\n",
    "其中，$c_{jj}$ 是 $(X^TX)^{-1}$ 中第 $(j, j)$ 元素。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
